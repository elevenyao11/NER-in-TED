## Annotation Plan

### 1. Introduction

In this annotation, we will be working on annotating named entities in the TED corpus. We will be especially focusing on the English corpus for now, but we will conduct cross-lingual projection based on the annotated English corpus in order to construct the parallel English-Korean-Chinese corpus during Week 4.

### 2. Annotation Process

The annotation process includes three major steps.

#### 2.1 Extract Named Entities Using spaCy and Store the Result in a Json File

First, we are going to extract named entities in the corpus using spaCy. More specifically, we will create json-structured dictionaries for each paragraph in the corpus. Each dictionary contains:
- text: A text of each paragraph
- ents: A list of entities in each paragraph. Each entity contains the start index, the end index, and the label.
- title: The title of each paragraph. By default, it is None for all paragraphs.

The example code for constructing the dictionary is as below:

```python
nlp = en_core_web_sm.load()
def extract_ne(paragraph):
    doc = nlp(paragraph)
    ne_dict = {}
    ne_dict["text"] = paragraph
    ents = []
    for ent in doc.ents:
        ent_dict = {}
        ent_dict["start"] = ent.start_char
        ent_dict["end"] = ent.end_char
        ent_dict["label"] = ent.label_
        ents.append(ent_dict)
    ne_dict["ents"] = ents
    ne_dict["title"] = None
    return ne_dict
```
Also, the output of the paragraph after applying the function will be as below:

```python
{'text': 'How do we know this? Well, in the last few decades, neuroscientists have made enormous breakthroughs in understanding how our brains work by monitoring them in real time with instruments like fMRI and PET scanners. When people are hooked up to these machines, tasks, such as reading or doing math problems, each have corresponding areas of the brain where activity can be observed. But when researchers got the participants to listen to music, they saw fireworks. Multiple areas of their brains were lighting up at once, as they processed the sound, took it apart to understand elements like melody and rhythm, and then put it all back together into unified musical experience. And our brains do all this work in the split second between when we first hear the music and when our foot starts to tap along.',
  'ents': [{'start': 30, 'end': 50, 'label': 'DATE'},
   {'start': 52, 'end': 67, 'label': 'NORP'},
   {'start': 201, 'end': 204, 'label': 'ORG'},
   {'start': 723, 'end': 729, 'label': 'CARDINAL'},
   {'start': 746, 'end': 751, 'label': 'ORDINAL'}],
  'title': None},
```

This json structure can be used for visualizing named entity annotations.

**![](https://lh5.googleusercontent.com/YVcwvEaL8SvHmtNZ60rlNN8NpTiSQtnZ_08gbnpaSDUOtBxJ77-3go9NryLlACAyujxErRzRr8b9qMMsbboKerfd94FuZmZsLgg5OtQCFcYAgiyJddoUmwE_RZAGYEmGV9glZovu)**

#### 2.2 Preparing a CSV file for AMT

The second step is preparing a CSV file in order to fix the errors of automatically-generated labels from spaCy. Specifically, we will use Mechanical Turk for correcting annotations.

In order to enhance the quality of the annotations, we will provide context around each entity.

For example, in this paragraph:

```
How do we know this? Well, in the last few decades, neuroscientists have made enormous breakthroughs in understanding how our brains work by monitoring them in real time with instruments like fMRI and PET scanners. When people are hooked up to these machines,
```
if we assign a task to confirm (or correct) the label of “**neuroscientists**”, the contexts around “**neuroscientists**”” will be offered as well as the current label.

| text_id | para_id | ent_id | before                          | entity          | after                                              | label |
|---------|---------|--------|---------------------------------|-----------------|----------------------------------------------------|-------|
| 0       | 1       | 3      | Well, in the last few decades,  | neuroscientists | have made enormous breakthroughs in understanding  | NORP  |

-   text_id: the index of the text where the entity is.
-   para_id: the index of the paragraph where the entity is.
-   ent_id: the index of the entity.
-   before: the context before the entity.
-   entity: the entity.
-   after: the context after the entity.
-   label: the current label of the entity generated by spaCy.

So, each annotator will decide whether the current label is correct for the entity. If it is correct, they will choose “yes”, and if it is incorrect, they will choose “no” along with the most appropriate label.

 The list of labels of named entity is as follows.
 
| Type        | Description                                          |
|-------------|------------------------------------------------------|
| PERSON      | People, including fictional.                         |
| NORP        | Nationalities or religious or political groups.      |
| FAC         | Buildings, airports, highways, bridges, etc.         |
| ORG         | Companies, agencies, institutions, etc.              |
| GPE         | Countries, cities, states.                           |
| LOC         | Non-GRE locations, mountain ranges, bodies of water. |
| PRODUCT     | Objects, vehicles, foods, etc (Not services.)        |
| EVENT       | Named hurricanes, battles, wars, sports events, etc. |
| WORK_OF_ART | Title of books, songs, etc.                          |
| LAW         | Named documents made into laws.                      |
| LANGUAGE    | Any named language.                                  |
| DATE        | Absolute or relative dates or periods.               |
| TIME        | Times smaller than a day.                            |
| PERCENT     | Percentage, including “%”.                           |
| MONEY       | Monetary values, including unit.                     |
| QUANTITY    | Measurements, as of weight or distance.              |
| ORDINAL     | “first”, “second”, etc.                              |
| CARDINAL    | Numerals that do not fall under another type.        |
| N/A         | If any of the above is applied.                      |
source: [spaCy](https://spacy.io/usage/linguistic-features#named-entities) 

#### 2.3 Update labels with the human-fixed labels

The last step of the annotation is to update automatically-generated labels with the human-fixed ones.  

After the annotation is completed by workers from AMS, we expect the structure of the annotated CSV file would be as below:

| text_id | para_id | ent_id | before                          | entity          | after                                              | label | annotation  |
|---------|---------|--------|---------------------------------|-----------------|----------------------------------------------------|-------|-------------|
| 0       | 1       | 3      | Well, in the last few decades,  | neuroscientists | have made enormous breakthroughs in understanding  | NORP  | NO: PERSON |

(**note**: “PERSON” might not be the correct annotation, but we will accept all the manual annotations by workers.)

Using text_id, para_id, and the ent_id, we will replace the annotation by spaCy with the annotation by human workers. For example,
```python
# corpus[text_id][para_id]["ents"][ent_id]["label"] = "new_label"
corpus[0][1]["ents"][3]["label"] = "PERSON"
```


### 3. Rough Expectation of the Amount of Data

Currently, the corpus contains 780 English transcripts. The total amount of data varies depending on the number of named entities in each paragraph. Therefore, we investigated the entire paragraph in order to estimate the amount of work required for annotation. 

In `ted_talks_en.txt`, we found 59,685 named entities annotated by spaCy. If we remove duplicates, the number reduces into 17,253. Also, we found that we do not actually need to correct all the errors across all categories because not all named entities are elements that we are focusing on in this project. So, we decided to correct named entities with the labels `{'PERSON', 'FAC', 'ORG', 'GPE', 'LOC', 'PRODUCT', 'EVENT', 'WORK_OF_ART', 'LANGUAGE'}`. Therefore, the final amount of assignments is around **9,500** assignments.  

That is, if we use three workers, we need to complete 28,500 assignments from AMT, which requires at least 285 dollars (0.01 dollars per assignment). This is not feasible at this moment given that we have a 50 dollar limit. Instead, we will only publish the first **1,650 entities** to AMT, and then the rest of the annotations will be left for later opportunities.



### 4. Expected Corpus with Annotation

Finally, the expected structure of the annotated corpus file is _a json file_ which has a structure as the example below.

```
[
    {'title': 'TEXT1',
     'paragraphs': [
                {'text': "It gets worse. The New York Post filed a Freedom of Information Act request, got all the teachers' names and all their scores and they published them as an act of teacher-shaming. When I tried to get the formulas, the source code, through the same means, I was told I couldn't. I was denied. I later found out that nobody in New York City had access to that formula. No one understood it. Then someone really smart got involved, Gary Rubinstein. He found 665 teachers from that New York Post data that actually had two scores. That could happen if they were teaching seventh grade math and eighth grade math. He decided to plot them. Each dot represents a teacher.",
                 'ents': [{'start': 15, 'end': 32, 'label': 'ORG'},
                  {'start': 39, 'end': 67, 'label': 'LAW'},
                  {'start': 325, 'end': 338, 'label': 'GPE'},
                  {'start': 429, 'end': 444, 'label': 'PERSON'},
                  {'start': 455, 'end': 458, 'label': 'CARDINAL'},
                  {'start': 478, 'end': 491, 'label': 'ORG'},
                  {'start': 515, 'end': 518, 'label': 'CARDINAL'},
                  {'start': 567, 'end': 574, 'label': 'ORDINAL'},
                  {'start': 590, 'end': 596, 'label': 'ORDINAL'}],
                 'title': None},
                 {'text': 'But it was. This is Sarah Wysocki. She got fired, along with 205 other teachers, from the Washington, DC school district, even though she had great recommendations from her principal and the parents of her kids.',
			     'ents': [{'start': 20, 'end': 33, 'label': 'PERSON'},
					      {'start': 61, 'end': 64, 'label': 'CARDINAL'},
					      {'start': 90, 'end': 100, 'label': 'GPE'}],
			     'title': None},
                   ...
                   ],
    },
    {'title': 'TITLE2',
     'paragraphs': [{'para1'},
				     {'para2'}, .
				     ..]
    },
    ...
]
```


### 5. A Pilot Study of Annotation

We conducted a pilot study of annotation by publishing [a csv file](https://github.ubc.ca/iameleve/COLX_523_Group2/blob/master/annotation/mturk_case_study.csv) to Amazon Mechanical Turk. The current csv file contains 41 examples, after removing duplicates and selecting a reduced amount of labels from [the originally generated csv file](https://github.ubc.ca/iameleve/COLX_523_Group2/blob/master/annotation/Annotation_final.csv). The current setting for AMT is as below.


- Number of Assignments per task: 3
- Reward per Assignment: $0.01
- Assignment duration: 3 minutes
- Auto Approval Delay: 3 days
- The total number of assignments: 163
- No other limitation for workers. 

After publishing a small batch, all assignments were done within 10 minutes, with an average time per assignment of 31 seconds. However, we had to reject some of the annotations by AMT workers due to mismatch between received annotations and expected results. Roughly, the rejection rate was around 25 percent, which was considerably high. Therefore, when we actually conduct the annotation task next week, we would only allow workers with an approval rate of 90 percent or above to be able to participate in the annotation. 