{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "valued-things",
   "metadata": {},
   "source": [
    "### Filter corpus\n",
    "- This code is for filtering the corpus files in different languages\n",
    "- Currently the number of talks(documents) and the number of paragraphs in each talk in three language corpora are different.\n",
    "- We should match those numbers for creating a parallel corpus\n",
    "- The current details about the corpus is in this [link](https://github.ubc.ca/iameleve/COLX_523_Group2/blob/master/Milestone_2/corpus_readme.md#5-corpus-details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-superior",
   "metadata": {},
   "source": [
    "### Prepare corpora in dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bored-concert",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "previous-advocate",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_file_path = \"../transcripts/en/annotated/annotated_ted_talks_en.json\"\n",
    "ko_file_path = \"../transcripts/ko/annotated/annotated_ted_talks_ko.json\"\n",
    "cn_file_path = \"../transcripts/zh-cn/annotated/annotated_ted_talks_cn.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "stainless-tooth",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(en_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    en_corpus = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "explicit-peeing",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ko_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    ko_corpus = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "christian-harvey",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cn_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    cn_corpus = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "divine-boulder",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_df = pd.DataFrame(en_corpus)\n",
    "ko_df = pd.DataFrame(ko_corpus)\n",
    "cn_df = pd.DataFrame(cn_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleased-sandwich",
   "metadata": {},
   "source": [
    "### First, filter talks\n",
    "- We are going to filter talks that appear in all three corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "environmental-disposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_title = en_df[[\"title\"]]\n",
    "ko_title = ko_df[[\"title\"]]\n",
    "cn_title = cn_df[[\"title\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "extra-consumption",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bill_joy_what_i_m_worried_about_what_i_m_excited_about'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_title[\"title\"][775]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "committed-marshall",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = en_title.merge(ko_title, on=\"title\", how=\"inner\").drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "brilliant-means",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x.merge(cn_title, on=\"title\", how=\"inner\").drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "thirty-access",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_corpus = y[\"title\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "sacred-lindsay",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "519"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(common_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "military-picking",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_df_filtered = en_df[en_df[\"title\"].isin(common_corpus)].drop_duplicates(\"title\")\n",
    "ko_df_filtered = ko_df[ko_df[\"title\"].isin(common_corpus)].drop_duplicates(\"title\")\n",
    "cn_df_filtered = cn_df[cn_df[\"title\"].isin(common_corpus)].drop_duplicates(\"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "contrary-gambling",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(en_df_filtered) == len(ko_df_filtered) == len(cn_df_filtered) == len(common_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-stability",
   "metadata": {},
   "source": [
    "### Second, we are going to filter those where the number of paragraphs is identical for each dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "foster-shaft",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column that identifies the number of paragraphs in each talk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "geological-animation",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_df_filtered[\"num_paras\"] = en_df_filtered[\"text\"].apply(len)\n",
    "ko_df_filtered[\"num_paras\"] = ko_df_filtered[\"text\"].apply(len)\n",
    "cn_df_filtered[\"num_paras\"] = cn_df_filtered[\"text\"].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "expected-tobago",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_paras = en_df_filtered[[\"title\", \"num_paras\"]].reset_index()\n",
    "ko_paras = ko_df_filtered[[\"title\", \"num_paras\"]].reset_index()\n",
    "cn_paras = cn_df_filtered[[\"title\", \"num_paras\"]].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "indie-popularity",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_paras[\"num_paras_ko\"] = ko_paras[\"num_paras\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "three-trustee",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_paras[\"num_paras_cn\"] = cn_paras[\"num_paras\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "educational-concrete",
   "metadata": {},
   "outputs": [],
   "source": [
    "para_common_list = en_paras.query(\"num_paras == num_paras_ko == num_paras_cn\")[\"title\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "approved-experiment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "330"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(para_common_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "progressive-tiffany",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_df_filtered_paras = en_df_filtered[en_df_filtered[\"title\"].isin(para_common_list)].drop_duplicates(\"title\")\n",
    "ko_df_filtered_paras = ko_df_filtered[ko_df_filtered[\"title\"].isin(para_common_list)].drop_duplicates(\"title\")\n",
    "cn_df_filtered_paras = cn_df_filtered[cn_df_filtered[\"title\"].isin(para_common_list)].drop_duplicates(\"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "rental-neighborhood",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(en_df_filtered_paras) == len(ko_df_filtered_paras) == len(cn_df_filtered_paras) == len(para_common_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "responsible-chuck",
   "metadata": {},
   "source": [
    "### Finally, let's take a look at the corpus:\n",
    "- How many documents are in each corpus?\n",
    "- How many paragraphs are in each corpus?\n",
    "- What is the average number of paragraphs?\n",
    "- How many tokens in each corpus?\n",
    "- What is the number of character for each corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "threaded-digest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330\n",
      "330\n",
      "330\n"
     ]
    }
   ],
   "source": [
    "# num_docs\n",
    "print(len(en_df_filtered_paras))\n",
    "print(len(ko_df_filtered_paras))\n",
    "print(len(cn_df_filtered_paras))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "bottom-giant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7660\n"
     ]
    }
   ],
   "source": [
    "# num_paras\n",
    "print(en_df_filtered_paras[\"num_paras\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "explicit-clock",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.21212121212121\n"
     ]
    }
   ],
   "source": [
    "# avg. num_paras\n",
    "print(en_df_filtered_paras[\"num_paras\"].sum() / len(en_df_filtered_paras))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "virgin-curtis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num. tokens/characters in each corpus\n",
    "def count_tokens(text):\n",
    "    num_tokens = 0\n",
    "    for para in text:\n",
    "        tokens = para[\"text\"].split()\n",
    "        num_tokens += len(tokens)\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "frank-geneva",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_df_filtered_paras[\"num_tokens\"] = en_df_filtered_paras[\"text\"].apply(count_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "friendly-disease",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "689614"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# num_tokens in eng corpus\n",
    "en_df_filtered_paras[\"num_tokens\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "cross-distinction",
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_df_filtered_paras[\"num_tokens\"] = ko_df_filtered_paras[\"text\"].apply(count_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "strategic-register",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "478551"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# num_tokens in kor corpus\n",
    "ko_df_filtered_paras[\"num_tokens\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "accredited-french",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_chars(text):\n",
    "    num_chars = 0\n",
    "    for para in text:\n",
    "        num_chars += len(para[\"text\"])\n",
    "    return num_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "sensitive-grade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3792843"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# num_chars in eng_corpus\n",
    "en_df_filtered_paras[\"num_chars\"] = en_df_filtered_paras[\"text\"].apply(count_chars)\n",
    "en_df_filtered_paras[\"num_chars\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "verbal-migration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2027123"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# num_chars in kor_corpus\n",
    "ko_df_filtered_paras[\"num_chars\"] = ko_df_filtered_paras[\"text\"].apply(count_chars)\n",
    "ko_df_filtered_paras[\"num_chars\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "further-bloom",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1264013"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# num_chars in cn_corpus\n",
    "cn_df_filtered_paras[\"num_chars\"] = cn_df_filtered_paras[\"text\"].apply(count_chars)\n",
    "cn_df_filtered_paras[\"num_chars\"].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-contract",
   "metadata": {},
   "source": [
    "### Save the filtered corpora to json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "capable-perception",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_filtered_file_path = \"../transcripts/en/filtered/filtered_annotated_ted_talks_en.json\"\n",
    "ko_filtered_file_path = \"../transcripts/ko/filtered/filtered_annotated_ted_talks_ko.json\"\n",
    "cn_filtered_file_path = \"../transcripts/zh-cn/filtered/filtered_annotated_ted_talks_cn.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "rapid-adapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_corpus_to_list(corpus):\n",
    "    return corpus.apply(lambda x:x.to_dict(), axis=1).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "female-monaco",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_corpus_list = convert_corpus_to_list(en_df_filtered_paras)\n",
    "ko_corpus_list = convert_corpus_to_list(ko_df_filtered_paras)\n",
    "cn_corpus_list = convert_corpus_to_list(cn_df_filtered_paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "individual-salem",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json(corpus, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(corpus, json_file,  indent=4, separators=(',', ':'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "dimensional-quality",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_json(en_corpus_list, en_filtered_file_path)\n",
    "write_json(ko_corpus_list, ko_filtered_file_path)\n",
    "write_json(cn_corpus_list, cn_filtered_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fancy-lambda",
   "metadata": {},
   "source": [
    "### Let's test out our filtered corpora are parallelized in paragraph-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "facial-entity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_parallel_corpus(en_df, ko_df, cn_df, title, para_id):\n",
    "    en_row = en_df.query(f\"title == '{title}'\")[\"text\"].to_list()[0][para_id][\"text\"]\n",
    "    ko_row = ko_df.query(f\"title == '{title}'\")[\"text\"].to_list()[0][para_id][\"text\"]\n",
    "    cn_row = cn_df.query(f\"title == '{title}'\")[\"text\"].to_list()[0][para_id][\"text\"]\n",
    "    return en_row, ko_row, cn_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "pediatric-edwards",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(en_filtered_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    en_corpus = json.load(f)\n",
    "\n",
    "with open(ko_filtered_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    ko_corpus = json.load(f)\n",
    "\n",
    "with open(cn_filtered_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    cn_corpus = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "hollow-yacht",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_df = pd.DataFrame(en_corpus)\n",
    "ko_df = pd.DataFrame(ko_corpus)\n",
    "cn_df = pd.DataFrame(cn_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "instructional-publisher",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('One day in New York, I was on the street and I saw some kids playing baseball between stoops and cars and fire hydrants. And a tough, slouchy kid got up to bat, and he took a swing and really connected. And he watched the ball fly for a second, and then he went, \"Dah dadaratatatah. Brah dada dadadadah.\" And he ran around the bases. And I thought, go figure. How did this piece of 18th century Austrian aristocratic entertainment turn into the victory crow of this New York kid? How was that passed on? How did he get to hear Mozart?',\n",
       " '하루는 제가 뉴욕에서 길을 걷고 있었는데 꼬마들이 현관과 자동차와 소화전들 사이에서 야구를 하고 있더군요 구부정하고 억세 보이는 한 꼬마가 자기 차례가 되자 야구배트를 휘둘렀는데, 정확히 딱 맞혔어요 그리고는 그 아이가 날아가는 공을 잠시 보더니 이러더군요 \"따따 따따 라따따\" \\'따따 따따 따따따따따;\" 이러며 베이스 사이를 뛰어다녔습니다 보고 있던 전 생각했죠 어떻게 18세기 오스트리아 귀족의 놀이 음악이 이 뉴욕 꼬마의 승리 행진곡이 될 수 있었을까? 그 음악이 어떻게 전해졌을까? 저 꼬마는 어디서 모차르트 음악을 들은 걸까?',\n",
       " '有一天，我在纽约的街头 我看到一些小孩子在门廊，汽车和消防栓之间打棒球。 一个强壮的，无精打采的孩子准备击球， 他甩开球棒，真的击到了球。 然后他看着球飞了一会儿， 然后就唱起来，”达 达达……（音乐旋律）。“ ”巴 达达 达……“ 然后他绕着球场跑起来。 我就想，试着猜猜看吧。 这首18世纪奥地利的贵族音乐 是怎么样变成这个纽约孩子的胜利的时候欢唱的旋律的？ 这首曲子是怎么样流传下来的？那个孩子是怎么样听到莫扎特的曲子的？')"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_parallel_corpus(en_df, ko_df, cn_df, para_common_list[6], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "urban-acting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"If you understand the difference between 'the world' and 'my world,' you understand the difference between logos and mythos. 'The world' is objective, logical, universal, factual, scientific. 'My world' is subjective. It's emotional. It's personal. It's perceptions, thoughts, feelings, dreams. It is the belief system that we carry. It's the myth that we live in.\",\n",
       " '\"세계\"와 \"세상\"의 차이를 이해하신다면 이성과 신화의 차이를 이해한 것입니다. \"세계\"는 객관적이고 논리적이고 보편적이며 사실적이고 과학적입니다. \"내 세상\"은 주관적이고 감성적이며 개인적입니다. 그것은 지각이고 생각이고 느낌이며 꿈입니다. 그것이 우리가 가지는 신념 체계입니다. 그것이 우리가 살고 있는 신화의 틀입니다.',\n",
       " '如果你明白了\"外在世界\"和\"内在世界\"的区别，你就明白了理性和神话的区别 如果你明白了\"外在世界\"和\"内在世界\"的区别，你就明白了理性和神话的区别 “外在世界”是客观公正，理性逻辑，普遍适用，实实在在，符合科学的 “外在世界”是客观公正，理性逻辑，普遍适用，实实在在，符合科学的 “外在世界”是客观公正，理性逻辑，普遍适用，实实在在，符合科学的 “内在世界”是主观的，感性的，个人的，它是你的观念、思想、感觉、梦想 “内在世界”是主观的，感性的，个人的，它是你的观念、思想、感觉、梦想 “内在世界”是主观的，感性的，个人的，它是你的观念、思想、感觉、梦想 “内在世界”是我们的信仰，是我们的世界观，是属于我们的神话世界 “内在世界”是我们的信仰，是我们的世界观，是属于我们的神话世界')"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_parallel_corpus(en_df, ko_df, cn_df, para_common_list[10], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sophisticated-meaning",
   "metadata": {},
   "source": [
    "Now they seem to be parallelized!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approximate-auckland",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
